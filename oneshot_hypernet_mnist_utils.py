# -*- coding: utf-8 -*-
"""meta_model_MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DGJHXDMn2uIO-F4bblglxQIHM4M8WXjz

## Author
This notebook and the underlying implementation were developed by **Halil Candan**  
All external sources used during this project have been properly cited.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR
import torch.nn.init as init

import torchvision
from torchvision.datasets import MNIST
from torchvision import datasets, transforms
from torch.utils.data import Subset

import matplotlib.pyplot as plt
import numpy as np
import random
from collections import Counter
from functools import reduce
import time
import optuna

"""
# Define a Class That Creates a Query Set When It is Called
## Class: OneShotMetaDataset
Here is what it does

### Initialization
1. Load MNIST data set (for example 0-5 MNIST set)
2. Groups image indices by class label using a dictionary, so that each key corresponds to a digit (e.g., 0–5), and its value is a list of indices for that class.

### xtr, query = plot_query(query_size) Function
1. Randomly selects a target class `c_tr`.
2. Picks one support sample `x_tr` from `c_tr`.
3. Selects `query_size // 2` positive examples (same class).
4. Selects `query_size // 2` negative examples (different classes).
5. Shuffles positives and negatives to form the query set.
6. Returns support sample (xt) and query samples with binary labels (1 = same class, 0 = different class).
"""

class OneShotMetaDataset:
    def __init__(self, dataset, device=None):
        self.device = device
        self.images = torch.stack([img for img, _ in dataset])
        self.labels = torch.tensor([label for _, label in dataset])

        self.classes = torch.unique(self.labels).tolist()
        self.indices_per_class = {c: torch.where(self.labels == c)[0] for c in self.classes}

    def create_query(self, query_size: int):
        c_tr = random.choice(self.classes)
        class_indices = self.indices_per_class[c_tr]

        perm = torch.randperm(len(class_indices))
        xtr_idx = class_indices[perm[0]].item()
        pos_indices = class_indices[perm[1:1 + query_size // 2]]

        xtr = (self.images[xtr_idx], self.labels[xtr_idx].item())

        neg_class_indices = torch.cat([self.indices_per_class[c] for c in self.classes if c != c_tr])
        perm = torch.randperm(len(neg_class_indices))
        neg_indices = neg_class_indices[perm[:query_size // 2]]

        all_indices = torch.cat([pos_indices, neg_indices])
        perm_all = torch.randperm(len(all_indices))
        shuffled_idx = all_indices[perm_all]

        query = [(self.images[i], lbl.item(), int(lbl.item() == c_tr)) for i, lbl in zip(shuffled_idx, self.labels[shuffled_idx])]

        if self.device is not None:
            xtr = (xtr[0].to(self.device), xtr[1])
            query = [(img.to(self.device), lbl, binary_lbl) for img, lbl, binary_lbl in query]
        return xtr, query

# Helper function for ploting suppor and query list
def plot_support_and_query(x_tr, query):
    x_img, x_label = x_tr

    # Plot x_tr image
    plt.figure(figsize=(14, 3))
    plt.subplot(1, len(query) + 1, 1)
    plt.imshow(x_img.squeeze().cpu(), cmap='gray')
    plt.title(f"Support\nLabel: {x_label}")
    plt.axis('off')

    # Plot query images
    for i in range(len(query)):
        img, true_label, binary_label = query[i]
        plt.subplot(1, len(query) + 1, i + 2)
        plt.imshow(img.squeeze().cpu(), cmap='gray')
        plt.title(f"Label: {true_label}\nY: {binary_label}")
        plt.axis('off')

    plt.tight_layout()
    plt.show()


"""
# The Network Architecture
## class Net(nn.Module)
### Initialization
1. Creates the network. Its architecture is showed above.

### sample_weights() Function
1. It passes embedding vector E to the HyperNetwork.
2. Hypernetwork produces weights (θ) of the main network.
3. Main network paramteres are then updated with the new parameters.

### forward() Function
1. It passes images into the main network

### Notes:
* This network codes are inspired by that [torch-hypernetwork-tutorials repo](https://github.com/qu-gg/torch-hypernetwork-tutorials).
* That repo only contains HyperNetwork and Main Network. It lacks feature extractor (CNN) or Glorot initialization. It helped me to understand how hypernetworks are working and give me a base code hence i can develop more complex algorithms.
"""

# Define the architecture of the neural network
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        # Feature extraction CNN
        self.feature_net = nn.Sequential(
          nn.Conv2d(1, 32, kernel_size=3, padding=1),   # → 32×28×28
          nn.BatchNorm2d(32),
          nn.ReLU(),
          nn.MaxPool2d(2),                              # → 32×14×14

          nn.Conv2d(32, 64, kernel_size=3, padding=1),  # → 64×14×14
          nn.BatchNorm2d(64),
          nn.ReLU(),
          nn.MaxPool2d(2),                              # → 64×7×7

          nn.Conv2d(64, 128, kernel_size=3, padding=1), # → 128×7×7
          nn.BatchNorm2d(128),
          nn.ReLU(),
          nn.MaxPool2d(2),                              # → 128×3×3

          nn.AdaptiveAvgPool2d(1),                      # → 128×1×1
          nn.Flatten(),                                 # → [B, 128]
          nn.Dropout(0.3)
        )
        feat_dim = 128

        # main-net layer dimensions
        self.layers_dim = [feat_dim, 64, 1]

        # Total main network parameters (weights + biases)
        # Matches hypernetwork output dimension
        self.main_param_cnt  = sum(
            [self.layers_dim[i] * self.layers_dim[i + 1] + self.layers_dim[i + 1]
             for i in range(len(self.layers_dim) - 1)])

        # Initialize embedding vector
        self.embedding = torch.zeros(1, feat_dim)

        # Full hypernetwork, takes embedding input and outputs weights of the main network
        self.hypernet = nn.Sequential(
          nn.Linear(feat_dim, 256), nn.ReLU(), nn.Dropout(0.2),
          nn.Linear(256, 256), nn.ReLU(), nn.Dropout(0.2),
          nn.Linear(256, self.main_param_cnt)
        )
        """
        # Glorot Initialization for HyperNetwork parameters
        for hlayer in self.hypernet:
            if isinstance(hlayer, nn.Linear):
              gain = init.calculate_gain('relu')
              init.xavier_normal_(hlayer.weight, gain=gain)
              if hlayer.bias is not None:
                  nn.init.zeros_(hlayer.bias)
        """

        # Main network structure (dummy initialization)
        # The actual weights will be dynamically generated by the hypernetwork
        self.main_net = nn.Sequential(
            nn.Linear(self.layers_dim[0], self.layers_dim[1]),
            nn.ReLU(),
            nn.Linear(self.layers_dim[1], self.layers_dim[2]),
        )

        # main_net weights should not be updated via backpropagation
        for p in self.main_net.parameters():
            p.requires_grad = False

    # Generates and assigns weights to the main network from the hypernetwork
    def sample_weights(self):
        # Get the main network parameters from the hypernetwork output
        self.hypernet_outputs = self.hypernet(self.embedding)[0]

        # Split and assign generated weights and biases to main_net layers
        next_idx = 0
        for i in range(len(self.layers_dim) - 1):
            in_dim = self.layers_dim[i]
            out_dim = self.layers_dim[i + 1]

            # Extract weight slice and reshape
            cur_idx = next_idx
            next_idx += in_dim * out_dim
            weights_slice = self.hypernet_outputs[cur_idx:next_idx].reshape([out_dim, in_dim])

            # Assign generated weights
            del self.main_net[i * 2].weight
            self.main_net[i * 2].weight = weights_slice

            # Extract bias slice and assign
            cur_idx = next_idx
            next_idx += out_dim
            bias_slice = self.hypernet_outputs[cur_idx:next_idx].reshape(out_dim)
            del self.main_net[i * 2].bias
            self.main_net[i * 2].bias = bias_slice

    def forward(self, x):
        # Extract features from input
        z = self.feature_net(x)

        # Flatten for fully-connected layers
        z = z.view(z.size(0), -1)

        # Predict using dynamically generated main_net
        return self.main_net(z)

"""# def train(model, device, obj_train_data, optimizer, iteration_count, query_size)

## Parameters:

* **model** (Net) — The model

* **device** (torch.device) — The target compute device i.e. "cuda" for GPU or "cpu" for CPU

* **obj_train_data** (OneShotMetaDataset) — OneShotMetaDataset object for creting support-query list

* **optimizer** (torch.optim.Optimizer) — The optimizer for the training

* **iteration_count** (int) — Number of training iterations. Each iteration gathers repeats episodes, aggregates their loss, and performs exactly one optimizer.step().

* **query_size** (int) — NUmbe of query images which are paired with the single support image in every one-shot episode.

<br>

## Training Procedure
In each training episode:

- A **support image** `x_tr` is sampled from digit classes `0–5`.
- A **query set** is created, containing both **positive** (same class) and **negative** (different class) examples.

- The **support image** is passed through the **feature extractor** (`feature_net`) to obtain an embedding.
- This embedding is given to the **hypernetwork**, which generates the parameters `θ` of the binary classifier (`main_net`).

- These parameters are **dynamically assigned** to the classifier (`main_net`), which is then applied to each query embedding.
- The classifier outputs **logits** representing how likely each query image belongs to the **same class as `x_tr`**.

- A **binary cross-entropy loss** is computed using the predicted logits and the ground-truth labels (`1` for same class, `0` for different).
- An optimizer step updates the parameters `γ` of the **hypernetwork** and the **feature extractor**.


## Notes:
- This train codes are inspired by that [torch-hypernetwork-tutorials repo](https://github.com/qu-gg/torch-hypernetwork-tutorials).

# def test(model, device, iteration_count, query_size,  obj_test_data, created_traindata)

## Parameters:

* **model** (Net) — Trained hypernetwork model whose accuracy will be measured.

* **device** (torch.device) — The target compute device i.e. "cuda" for GPU or "cpu" for CPU.

* **iteration_count** (int) — Number of test iterations.

* **query_size** (`int`) — Number of query images paired with the single support
  image in every evaluation episode.

* **obj_test_data** (`OneShotMetaDataset`, *optional*) —  Query generator used at test-time. If provided, the function calls  
  `obj_test_data.create_query(query_size)` on-the-fly for each iterations.

* **created_traindata** (`list`, *optional*) —  A pre-generated list for `(x_tr, query list)` pairs. Used when `obj_test_data` is `None`.


At Each iteration
samples one support image, its query set, updates main network parameters, passes query set and gives model accuracy

<br>

## Testing Procedure
- Each test iteration samples:
  - **One support image** from a randomly chosen test class (6–9)
  - A **query set** consisting of:
    - **8 positive examples** (same class as the support image)
    - **8 negative examples** (randomly chosen from different classes)

- The support image is passed through the `feature_net` and `hypernet` to generate new classifier parameters `θ`
- The `main_net` parameters are updated via `hypernet` parameters `θ`
- The `main_net` is then used to classify new (query) images:  
  It predicts **1** if a query image belongs to the same class (digit) as the support image, and **0** if it belongs to a different class.

- After the iteration loop, compute final accuracy
   ```python
   acc = 100.0 * total_correct / total_queries"
   return acc
   ```


## Notes:
- This train codes are inspired by that [torch-hypernetwork-tutorials repo](https://github.com/qu-gg/torch-hypernetwork-tutorials).
"""

def train(model, device, obj_train_data, optimizer, iteration_count, query_size):
    model.train()

    train_object = obj_train_data
    repeats=1 # M value

    for i in range(iteration_count):

      all_logits = []
      all_labels = []

      for episode in range(repeats):

        # Draw a support example x_tr and a balanced query set Q
        # Half of Q has the same class as x_tr (positives),
        # the other half a different class (negatives)
        (xtr_img, xtr_label), query = train_object.create_query(query_size)

        xtr_img = xtr_img.to(device)

        query_imgs = torch.stack([img for img,_,_ in query]).to(device)
        query_y_label = torch.tensor([b for _,_,b in query], dtype=torch.float32, device=device)

        # x_tr -> CNN -> HyperNet -> Update main net param
        feat = model.feature_net(xtr_img.unsqueeze(0).to(device))
        model.embedding = feat.view(1, -1)
        model.sample_weights() # Update main_net weights using hypernet

        # Forward pass query images
        logits = model(query_imgs).squeeze()
        all_logits.append(logits)
        all_labels.append(query_y_label)

      all_logits = torch.cat(all_logits, dim=0)
      all_labels = torch.cat(all_labels, dim=0)

      # Contruct losss
      loss = F.binary_cross_entropy_with_logits(all_logits, all_labels)

      # Model update for only CNN and HyperNet
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

def test(model, device, iteration_count, query_size,  obj_test_data=None, created_traindata=None):
    model.eval()

    total_correct = 0
    total_queries = 0

    # Determine which data sapmling source to use: dynamic generator or pre-created list
    if obj_test_data is not None:
        test_object = obj_test_data
    else:
      assert iteration_count <= len(created_traindata), f"iteration_count ({iteration_count}) cannot be greater than the number of available samples ({len(created_traindata)})."

    with torch.no_grad(): # Disable gradient tracking
        for i in range(iteration_count):

          # Sample one test episode: 1 support image + query set
          (xte_img,_), query = test_object.create_query(query_size) if (obj_test_data is not None) else created_traindata[i]

          xte_img = xte_img.to(device)

          query_imgs = torch.stack([img for img,_,_ in query])
          query_imgs = query_imgs.to(device)
          query_y_label = torch.tensor([b for _,_,b in query], dtype=torch.float32, device=device)

          # x_te -> CNN -> HyperNet -> Update main net param
          feat = model.feature_net(xte_img.unsqueeze(0).to(device))
          model.embedding = feat.view(1, -1)
          model.sample_weights() # Update main_net weights using hypernet

          # Forward pass query images through main_net
          logits = model(query_imgs).squeeze()

          # Binary predictions: threshold at 0
          preds  = (logits > 0).float()

          # Update correct prediction count
          total_correct += (preds == query_y_label).sum().item()
          total_queries += query_size

    # Compute final accuracy
    acc = 100. * total_correct / total_queries
    # print(f"\nOne-shot on digits 6–9 over {iteration_count} episodes:" f"  {total_correct}/{total_queries} correct → {acc:.2f}%\n")

    return acc

"""
#  Optional Sections
---
<br>

The following sections are **optional** and provide two different ways to interact with the model:

#### 1. Running Optuna Hyperparameter Optimization
This section allows the user to run Optuna to automatically find the optimal set of hyperparameters  
(e.g., learning rate, decay factor, number of iterations per epoch).  
It is useful when you want to explore the search space and improve performance systematically.

> ⚠️ Warning: This process may take a long time depending on the number of trials and device used.

#### 2. Manual Training with Custom Hyperparameters
This section allows the user to manually define their own hyperparameters   and train the model accordingly.  
It is useful for running experiments with specific configurations or reproducing a known setup.

You can freely modify values such as:
- `learning_rate`
- `gamma` (learning rate decay factor)
- `iteration_count` (number of one-shot episodes per epoch)
- total `n_epochs`

Both options are fully independent and can be used depending on your needs.

### Optuna Objective Function for Hyperparameter Optimization

The following `objective()` function is used to optimize key training hyperparameters using the **Optuna** framework.  
The goal is to maximize the **test accuracy** of the one-shot learning model on unseen MNIST classes.

#### Search Space:
- `gamma`: Learning rate decay factor for the scheduler (range: 0.70 – 0.99)
- `iter_count`: Number of one-shot episodes per epoch (2000–14000)
- `lr_adadelta`: Learning rate for the **Adadelta** optimizer (0.01–2.0, step=0.005)

#### Dynamic Epoch Calculation:
- The number of epochs is dynamically computed based on a fixed total training effort (`MAX_ITERS = 140,000`)  
  so that each trial trains approximately the same number of samples, regardless of iteration count.

#### Optimization Strategy:
- The `Net()` model is instantiated fresh in every trial and trained with the sampled hyperparameters.
- An `ExponentialLR` scheduler is applied using the trial's `gamma` value.
- After each epoch, the model is evaluated using a fixed test set (`test_queries`) to compute one-shot accuracy.
- The best accuracy seen during training is tracked and used as the **objective value** to be maximized.

#### Early Stopping (Pruning):
- If, after half the epochs, the best accuracy is still below 67%, the trial is reported as underperforming.
- In such cases, **Optuna's pruning mechanism** can terminate the trial early to save computation.

This setup enables a flexible and efficient search over a meaningful hyperparameter space.

<br><br>
This codes are based on the follwing links: <br>
[Mastering Hyperparameter Tuning with Optuna: Boost Your Machine Learning Models!](https://youtu.be/t-INgABWULw?si=AdrwXlFZ-SQ-YevE)

[Master the Power of Optuna: A Step-by-Step Guide](https://medium.com/data-and-beyond/master-the-power-of-optuna-a-step-by-step-guide-ed43500e9b95)
"""

def objective(trial):
    # Hyperparameters
    gamma= trial.suggest_float("gamma", 0.70, 0.99)
    iter_count = trial.suggest_int("iter_count",2000,14000,step=1000)
    lr = trial.suggest_float("lr_adadelta", 0.01, 2.0, step=0.005)

    # Dynamic Epochs
    MAX_ITERS = 140_000
    raw_epochs = round(MAX_ITERS / iter_count)
    n_epochs = max(min(raw_epochs, 50),10)

    # Seed for reproductivity
    seed = 42
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark     = False
    torch.use_deterministic_algorithms(False)

    # Model, optimizer, scheduler
    model = Net().to(device)

    opt_params = list(model.feature_net.parameters()) + list(model.hypernet.parameters())
    optimizer = optim.Adadelta(opt_params, lr=lr)
    scheduler = ExponentialLR(optimizer, gamma=gamma)

    best_acc = 0
    # Test and train
    for epoch in range(n_epochs):
        train(model, device, obj_train_data,
              optimizer, iteration_count=iter_count, query_size=16)

        val_acc = test(model, device, iteration_count=1000, query_size=16, obj_test_data=None, created_traindata=test_queries)

        best_acc = max(best_acc, val_acc)

        if epoch > (n_epochs // 2) and best_acc < 67:
            trial.report(best_acc, step=epoch)
            if trial.should_prune():
                raise optuna.TrialPruned()

        scheduler.step()

    return best_acc           # maximize